{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d0c6818",
   "metadata": {},
   "source": [
    "# Project 3: Making an AI agent with LLAMAINDEX - FunctionTool\n",
    "\n",
    "### Follow the instructions \n",
    "Here are [instructions](https://huggingface.co/learn/agents-course/unit2/llama-index/tools) for the tutorial that are helpful for this notebook.\n",
    "\n",
    "\n",
    "### Description:\n",
    "This project uses `llamaindex`, a library that provides a framework for developing your agents with ease.\n",
    "\n",
    "We look at Tools here. \n",
    "\n",
    "`Tools` are the fundamental objects used to build the agents.\n",
    "\n",
    "\n",
    "\n",
    "### For this course, I am using \n",
    "\n",
    "\n",
    "see also [github code](https://github.com/huggingface/agents-course)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e610a1ff",
   "metadata": {},
   "source": [
    "## Load Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7109b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import chromadb\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "\n",
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.tools.google import GmailToolSpec\n",
    "from llama_index.tools.mcp import BasicMCPClient, McpToolSpec\n",
    "from llama_index.core.agent.workflow import FunctionAgent, ToolCallResult, ToolCall\n",
    "from llama_index.core.workflow import Context\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb83118",
   "metadata": {},
   "source": [
    "##### Login to the Hugging Face Hub to Have Access to the Serveless Inference API\n",
    "\n",
    "Note: You will need to add your token when prompted.\n",
    "\n",
    "\n",
    "`HF_TOKEN_INFERENCE2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66e0ec6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables from .env\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0187b847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Hugging Face Token\n",
    "HF_TOKEN_INFERENCE2 = os.environ.get(\"HF_TOKEN_INFERENCE2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9268074b",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Creating a FunctionalTool\n",
    "\n",
    "A FunctionTool provides a simple way to wrap any Python function and make it available to an agent. You can pass either a synchronous or asynchronous function to the tool, along with optional name and `description` parameters. The name and description are particularly important as they help the agent understand when and how to use the tool effectively. Let’s look at how to create a FunctionTool below and then call it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc534e3",
   "metadata": {},
   "source": [
    "Awesome, now we have a local directory with all the personas that will be attending the party, we can load and index!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91c21d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting weather for New York\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ToolOutput(blocks=[TextBlock(block_type='text', text='The weather in New York is sunny')], tool_name='my_weather_tool', raw_input={'args': ('New York',), 'kwargs': {}}, raw_output='The weather in New York is sunny', is_error=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Useful for getting the weather for a given location.\"\"\"\n",
    "    print(f\"Getting weather for {location}\")\n",
    "    return f\"The weather in {location} is sunny\"\n",
    "\n",
    "tool = FunctionTool.from_defaults(\n",
    "    get_weather,\n",
    "    name=\"my_weather_tool\",\n",
    "    description=\"Useful for getting the weather for a given location.\",\n",
    ")\n",
    "tool.call(\"New York\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc91388e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Creating a QueryEngineTool\n",
    "\n",
    "The QueryEngine we defined in the previous unit can be easily transformed into a tool using the QueryEngineTool class. Let’s see how to create a QueryEngineTool from a QueryEngine in the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85b9cf21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    }
   ],
   "source": [
    "embed_model = HuggingFaceEmbedding(\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "db = chromadb.PersistentClient(path=\"./alfred_chroma_db\")\n",
    "chroma_collection = db.get_or_create_collection(\"alfred\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(vector_store, embed_model=embed_model)\n",
    "\n",
    "llm = HuggingFaceInferenceAPI(model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\")\n",
    "query_engine = index.as_query_engine(llm=llm)\n",
    "tool = QueryEngineTool.from_defaults(query_engine, name=\"some useful name\", description=\"some useful description\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77fe8e74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_index.core.tools.query_engine.QueryEngineTool at 0x3222777d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09470af3",
   "metadata": {},
   "source": [
    "---\n",
    "## Creating Toolspecs\n",
    "\n",
    "Think of ToolSpecs as collections of tools that work together harmoniously - like a well-organized professional toolkit. Just as a mechanic’s toolkit contains complementary tools that work together for vehicle repairs, a ToolSpec combines related tools for specific purposes. For example, an accounting agent’s ToolSpec might elegantly integrate spreadsheet capabilities, email functionality, and calculation tools to handle financial tasks with precision and efficiency.\n",
    "\n",
    "Install the Google Toolspec\n",
    "And now we can load the toolspec and convert it to a list of tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03f1d07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_spec = GmailToolSpec()\n",
    "tool_spec_list = tool_spec.to_tool_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db71d336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<llama_index.core.tools.function_tool.FunctionTool at 0x322867620>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x3228676e0>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x3228676b0>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x3228677a0>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x322867680>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x322867770>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_spec_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2803c6cb",
   "metadata": {},
   "source": [
    "To get a more detailed view of the tools, we can take a look at the metadata of each tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4338097f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('load_data',\n",
       "  \"load_data() -> List[llama_index.core.schema.Document]\\nLoad emails from the user's account.\"),\n",
       " ('search_messages',\n",
       "  \"search_messages(query: str, max_results: Optional[int] = None)\\n\\n        Searches email messages given a query string and the maximum number\\n        of results requested by the user\\n           Returns: List of relevant message objects up to the maximum number of results.\\n\\n        Args:\\n            query (str): The user's query\\n            max_results (Optional[int]): The maximum number of search results\\n            to return.\\n\\n        \"),\n",
       " ('create_draft',\n",
       "  \"create_draft(to: Optional[List[str]] = None, subject: Optional[str] = None, message: Optional[str] = None) -> str\\n\\n        Create and insert a draft email.\\n           Print the returned draft's message and id.\\n           Returns: Draft object, including draft id and message meta data.\\n\\n        Args:\\n            to (Optional[str]): The email addresses to send the message to\\n            subject (Optional[str]): The subject for the event\\n            message (Optional[str]): The message for the event\\n\\n        \"),\n",
       " ('update_draft',\n",
       "  \"update_draft(to: Optional[List[str]] = None, subject: Optional[str] = None, message: Optional[str] = None, draft_id: str = None) -> str\\n\\n        Update a draft email.\\n           Print the returned draft's message and id.\\n           This function is required to be passed a draft_id that is obtained when creating messages\\n           Returns: Draft object, including draft id and message meta data.\\n\\n        Args:\\n            to (Optional[str]): The email addresses to send the message to\\n            subject (Optional[str]): The subject for the event\\n            message (Optional[str]): The message for the event\\n            draft_id (str): the id of the draft to be updated\\n\\n        \"),\n",
       " ('get_draft',\n",
       "  \"get_draft(draft_id: str = None) -> str\\n\\n        Get a draft email.\\n           Print the returned draft's message and id.\\n           Returns: Draft object, including draft id and message meta data.\\n\\n        Args:\\n            draft_id (str): the id of the draft to be updated\\n\\n        \"),\n",
       " ('send_draft',\n",
       "  \"send_draft(draft_id: str = None) -> str\\n\\n        Sends a draft email.\\n           Print the returned draft's message and id.\\n           Returns: Draft object, including draft id and message meta data.\\n\\n        Args:\\n            draft_id (str): the id of the draft to be updated\\n\\n        \")]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(tool.metadata.name, tool.metadata.description) for tool in tool_spec_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c747fca1",
   "metadata": {},
   "source": [
    "## Model Context Protocol `(MCP)` in LlamaIndex\n",
    "\n",
    "LlamaIndex also allows using MCP tools through a [ToolSpec on the LlamaHub](https://llamahub.ai/l/tools/llama-index-tools-mcp?from=). You can simply run an MCP server and start using it through the following implementation.\n",
    "\n",
    "\n",
    "Install the MCP Toolspec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-agent-hugging-face-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
