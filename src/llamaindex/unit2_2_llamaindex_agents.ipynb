{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d0c6818",
   "metadata": {},
   "source": [
    "# Project 4: Making an AI agent with LLAMAINDEX - Agents\n",
    "\n",
    "### Follow the instructions \n",
    "Here are [instructions](https://huggingface.co/learn/agents-course/unit2/llama-index/agents) for the tutorial that are helpful for this notebook.\n",
    "\n",
    "\n",
    "### Description:\n",
    "This project uses `llamaindex`, a library that provides a framework for developing your agents with ease.\n",
    "\n",
    "We look at Agents here. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### For this course, I am using \n",
    "\n",
    "\n",
    "see also [github code](https://github.com/huggingface/agents-course)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e610a1ff",
   "metadata": {},
   "source": [
    "## Load Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7109b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "from llama_index.core.agent.workflow import AgentWorkflow,ToolCallResult, AgentStream\n",
    "from llama_index.core.tools import FunctionTool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb83118",
   "metadata": {},
   "source": [
    "##### Login to the Hugging Face Hub to Have Access to the Serveless Inference API\n",
    "\n",
    "Note: You will need to add your token when prompted.\n",
    "\n",
    "\n",
    "`HF_TOKEN_INFERENCE2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e0ec6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0187b847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Hugging Face Token\n",
    "HF_TOKEN_INFERENCE2 = os.environ.get(\"HF_TOKEN_INFERENCE2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51522057",
   "metadata": {},
   "outputs": [],
   "source": [
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9268074b",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Initializing Agents\n",
    "\n",
    "To create an agent, we start by providing it with a set of functions/tools that define its capabilities. Letâ€™s look at how to create an agent with some basic tools. As of this writing, the agent will automatically use the function calling API (if available), or a standard ReAct agent loop.\n",
    "\n",
    "LLMs that support a tools/functions API are relatively new, but they provide a powerful way to call tools by avoiding specific prompting and allowing the LLM to create tool calls based on provided schemas.\n",
    "\n",
    "ReAct agents are also good at complex reasoning tasks and can work with any LLM that has chat or text completion capabilities. They are more verbose, and show the reasoning behind certain actions that they take.\n",
    "\n",
    "### We will use the basic `AgentWorkflow` class to create an agent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc534e3",
   "metadata": {},
   "source": [
    "Awesome, now we have a local directory with all the personas that will be attending the party, we can load and index!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c21d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define sample Tool -- type annotations, function names, and docstrings, are all included in parsed schemas!\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two numbers\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "def subtract(a: int, b: int) -> int:\n",
    "    \"\"\"Subtract two numbers\"\"\"\n",
    "    return a - b\n",
    "\n",
    "\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "def divide(a: int, b: int) -> int:\n",
    "    \"\"\"Divide two numbers\"\"\"\n",
    "    return a / b\n",
    "\n",
    "# initialize llm\n",
    "llm = HuggingFaceInferenceAPI(model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\")\n",
    "\n",
    "# initialize agent\n",
    "agent = AgentWorkflow.from_tools_or_functions(\n",
    "    tools_or_functions=[subtract, multiply, divide, add],\n",
    "    llm=llm,\n",
    "    system_prompt=\"You are a math agent that can add, subtract, multiply, and divide numbers using provided tools.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc91388e",
   "metadata": {},
   "source": [
    "Then, we can run the agent and get the response and reasoning behind the tool calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ae6ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = agent.run(\"What is (2 + 2) * 2?\")\n",
    "async for ev in handler.stream_events():\n",
    "    if isinstance(ev, ToolCallResult):\n",
    "        print(\"\")\n",
    "        print(\"Called tool: \", ev.tool_name, ev.tool_kwargs, \"=>\", ev.tool_output)\n",
    "    elif isinstance(ev, AgentStream):  # showing the thought process\n",
    "        print(ev.delta, end=\"\", flush=True)\n",
    "\n",
    "resp = await handler\n",
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21defb30",
   "metadata": {},
   "source": [
    "In a similar fashion, we can pass state and context to the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edbd219",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import Context\n",
    "\n",
    "ctx = Context(agent)\n",
    "\n",
    "response = await agent.run(\"My name is Bob.\", ctx=ctx)\n",
    "response = await agent.run(\"What was my name again?\", ctx=ctx)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f460388",
   "metadata": {},
   "source": [
    "# Creating RAG Agents with QueryEngineTools\n",
    "\n",
    "Let's now re-use the `QueryEngine` we defined in the [previous unit on tools](https://cdn-notebooks.hf.co/tools.ipynb) and convert it into a `QueryEngineTool`. We will pass it to the `AgentWorkflow` class to create a RAG agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22eb37c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "\n",
    "# Create a vector store\n",
    "db = chromadb.PersistentClient(path=\"./alfred_chroma_db\")\n",
    "chroma_collection = db.get_or_create_collection(\"alfred\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "# Create a query engine\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "llm = HuggingFaceInferenceAPI(model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\")\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=vector_store, embed_model=embed_model\n",
    ")\n",
    "query_engine = index.as_query_engine(llm=llm)\n",
    "query_engine_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=query_engine,\n",
    "    name=\"personas\",\n",
    "    description=\"descriptions for various types of personas\",\n",
    "    return_direct=False,\n",
    ")\n",
    "\n",
    "# Create a RAG agent\n",
    "query_engine_agent = AgentWorkflow.from_tools_or_functions(\n",
    "    tools_or_functions=[query_engine_tool],\n",
    "    llm=llm,\n",
    "    system_prompt=\"You are a helpful assistant that has access to a database containing persona descriptions. \",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac46bd57",
   "metadata": {},
   "source": [
    "And, we can once more get the response and reasoning behind the tool calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd72421",
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = query_engine_agent.run(\n",
    "    \"Search the database for 'science fiction' and return some persona descriptions.\"\n",
    ")\n",
    "async for ev in handler.stream_events():\n",
    "    if isinstance(ev, ToolCallResult):\n",
    "        print(\"\")\n",
    "        print(\"Called tool: \", ev.tool_name, ev.tool_kwargs, \"=>\", ev.tool_output)\n",
    "    elif isinstance(ev, AgentStream):  # showing the thought process\n",
    "        print(ev.delta, end=\"\", flush=True)\n",
    "\n",
    "resp = await handler\n",
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b06875",
   "metadata": {},
   "source": [
    "## Creating multi-agent systems\n",
    "\n",
    "We can also create multi-agent systems by passing multiple agents to the `AgentWorkflow` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02481eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent.workflow import (\n",
    "    AgentWorkflow,\n",
    "    ReActAgent,\n",
    ")\n",
    "\n",
    "# Define some tools\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two numbers.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "def subtract(a: int, b: int) -> int:\n",
    "    \"\"\"Subtract two numbers.\"\"\"\n",
    "    return a - b\n",
    "\n",
    "\n",
    "# Create agent configs\n",
    "# NOTE: we can use FunctionAgent or ReActAgent here.\n",
    "# FunctionAgent works for LLMs with a function calling API.\n",
    "# ReActAgent works for any LLM.\n",
    "calculator_agent = ReActAgent(\n",
    "    name=\"calculator\",\n",
    "    description=\"Performs basic arithmetic operations\",\n",
    "    system_prompt=\"You are a calculator assistant. Use your tools for any math operation.\",\n",
    "    tools=[add, subtract],\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "query_agent = ReActAgent(\n",
    "    name=\"info_lookup\",\n",
    "    description=\"Looks up information about XYZ\",\n",
    "    system_prompt=\"Use your tool to query a RAG system to answer information about XYZ\",\n",
    "    tools=[query_engine_tool],\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "# Create and run the workflow\n",
    "agent = AgentWorkflow(agents=[calculator_agent, query_agent], root_agent=\"calculator\")\n",
    "\n",
    "# Run the system\n",
    "handler = agent.run(user_msg=\"Can you add 5 and 3?\")\n",
    "async for ev in handler.stream_events():\n",
    "    if isinstance(ev, ToolCallResult):\n",
    "        print(\"\")\n",
    "        print(\"Called tool: \", ev.tool_name, ev.tool_kwargs, \"=>\", ev.tool_output)\n",
    "    elif isinstance(ev, AgentStream):  # showing the thought process\n",
    "        print(ev.delta, end=\"\", flush=True)\n",
    "\n",
    "resp = await handler\n",
    "resp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-agent-hugging-face-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
