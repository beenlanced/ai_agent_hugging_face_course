{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f9f914",
   "metadata": {},
   "source": [
    "# Project 2 - Building Langgraph Model - Document Analysis Graph\n",
    "\n",
    "\n",
    "### Follow the instructions \n",
    "Here are [instructions](https://huggingface.co/learn/agents-course/unit2/langgraph/first_graph) for the tutorial that are helpful for this notebook.\n",
    "\n",
    "\n",
    "### Description:\n",
    "This project uses `langgraph`, a library that provides a framework for developing your agents with ease.\n",
    "\n",
    "We look at Agents here. \n",
    "\n",
    "### The Problem this Notebook Attempts to Solve\n",
    "\n",
    "Let’s create a document analysis system using LangGraph to serve Mr. Wayne’s needs. This system can:\n",
    "\n",
    "1. Process images document\n",
    "\n",
    "2. Extract text using vision models (Vision Language Model)\n",
    "\n",
    "3. Perform calculations when needed (to demonstrate normal tools)\n",
    "\n",
    "4. Analyze content and provide concise summaries\n",
    "\n",
    "5. Execute specific instructions related to documents\n",
    "\n",
    "\n",
    "### For this course, I am using \n",
    "\n",
    "\n",
    "see also [github code](https://github.com/huggingface/agents-course)\n",
    "\n",
    "Specifically, I look at the [course notebook](https://huggingface.co/agents-course/notebooks/blob/main/unit2/langgraph/agent.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c591b1",
   "metadata": {},
   "source": [
    "## Load Imports\n",
    "\n",
    " `LangGraph` provides the graph structure, while `LangChain` offers convenient interfaces for working with LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8f275e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import os\n",
    "from typing import List, TypedDict, Annotated, Optional\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619c2221",
   "metadata": {},
   "source": [
    "### This notebook relies on the idea of (Reason-Act-Observe) - `ReAct`\n",
    "\n",
    "As seen in the Unit 1, an agent needs 3 steps as introduced in the [ReAct](https://react-lm.github.io/) architecture : ReAct, a general agent architecture.\n",
    "\n",
    "* `reason` - about documents and requests. Let the model reason about the tool output to decide what to do next (e.g., call another tool or just respond directly) \n",
    "\n",
    "* `act` - let the model call specific tools that are appropriate for the task\n",
    "\n",
    "* `observe` - the results, i.e., pass the tool output back to the model\n",
    "\n",
    "*  `repeat` as necessary until needs have been fully addressed (the loop part of it all)\n",
    "\n",
    "\n",
    "\n",
    "<p>\n",
    "  <img alt=\"Langgraph ReAct Figure\" src=\"/imgs/langgraph_react.png\"/>\n",
    "</p>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c4699d",
   "metadata": {},
   "source": [
    "### Preparing Tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef6cbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your OpenAI API key here\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-xxxxx\"  # Replace with your actual API key\n",
    "\n",
    "# Initialize our vision LLM to handle image documents\n",
    "vision_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0) #temperature means here to be very precise\n",
    "\n",
    "# Initalize text LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102d2aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(img_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract text from an image file using a multimodal model.\n",
    "\n",
    "    Args:\n",
    "        img_path: A local image file path (strings).\n",
    "\n",
    "    Returns:\n",
    "        A single string containing the concatenated text extracted from each image.\n",
    "    \"\"\"\n",
    "    all_text = \"\"\n",
    "    try:\n",
    "\n",
    "        # Read image and encode as base64\n",
    "        with open(img_path, \"rb\") as image_file:\n",
    "            image_bytes = image_file.read()\n",
    "\n",
    "        image_base64 = base64.b64encode(image_bytes).decode(\"utf-8\")\n",
    "\n",
    "        # Prepare the prompt including the base64 image data\n",
    "        message = [\n",
    "            HumanMessage(\n",
    "                content=[\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": (\n",
    "                            \"Extract all the text from this image. \"\n",
    "                            \"Return only the extracted text, no explanations.\"\n",
    "                        ),\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/png;base64,{image_base64}\"\n",
    "                        },\n",
    "                    },\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # Call the vision-capable model\n",
    "        response = vision_llm.invoke(message)\n",
    "\n",
    "        # Append extracted text\n",
    "        all_text += response.content + \"\\n\\n\"\n",
    "\n",
    "        return all_text.strip()\n",
    "    except Exception as e:\n",
    "        # You can choose whether to raise or just return an empty string / error message\n",
    "        error_msg = f\"Error extracting text: {str(e)}\"\n",
    "        print(error_msg)\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "\n",
    "def divide(a: int, b: int) -> float:\n",
    "    \"\"\"Divide a and b.\"\"\"\n",
    "    return a / b\n",
    "\n",
    "\n",
    "tools = [\n",
    "    divide,\n",
    "    extract_text\n",
    "]\n",
    "llm_with_tools = llm.bind_tools(tools, parallel_tool_calls=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f61862",
   "metadata": {},
   "source": [
    "### Create LLM and Prompt It with the Overall Desired Agent Behavior\n",
    "\n",
    "#### This Agent's States are More Complex\n",
    "This state is a little more complex than the previous ones we have seen. **AnyMessage** is a class from Langchain that defines messages, and **add_messages** is an operator that adds the latest message rather than overwriting it with the latest state.\n",
    "\n",
    "This is a new concept in `LangGraph`, where you can add operators in your state to define the way they should interact together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab881e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    # The input document\n",
    "    input_file: Optional[str]  # Contains file path, type (PNG)\n",
    "    messages: Annotated[list[AnyMessage], add_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50229f1b",
   "metadata": {},
   "source": [
    "###  Create the Nodes \n",
    "\n",
    "\n",
    "##### First Define Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb796a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assistant(state: AgentState):\n",
    "    # System message\n",
    "    textual_description_of_tool = \"\"\"\n",
    "extract_text(img_path: str) -> str:\n",
    "    Extract text from an image file using a multimodal model.\n",
    "\n",
    "    Args:\n",
    "        img_path: A local image file path (strings).\n",
    "\n",
    "    Returns:\n",
    "        A single string containing the concatenated text extracted from each image.\n",
    "divide(a: int, b: int) -> float:\n",
    "    Divide a and b\n",
    "\"\"\"\n",
    "    image = state[\"input_file\"]\n",
    "    sys_msg = SystemMessage(content=f\"You are an helpful agent that can analyse some images and run some computatio without provided tools :\\n{textual_description_of_tool} \\n You have access to some optional images. Currently the loaded images is : {image}\")\n",
    "\n",
    "    return {\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])], \"input_file\": state[\"input_file\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316131ae",
   "metadata": {},
   "source": [
    "### Building Nodes\n",
    "\n",
    "We define a **tools** node with our list of tools.\n",
    "\n",
    "The **assistant** node is just our model with bound tools.\n",
    "\n",
    "We create a graph with **assistant** and **tools** nodes.\n",
    "\n",
    "We add **tools_condition** edge, which routes to **End** or to **tools** based on whether the **assistant** calls a tool.\n",
    "\n",
    "Now, we add one new step:\n",
    "\n",
    "We connect the **tools** node back to the **assistant**, forming a loop.\n",
    "\n",
    "* After the **assistant** node executes, **tools_condition** checks if the model's output is a tool call.\n",
    "\n",
    "* If it is a tool call, the flow is directed to the **tools** node.\n",
    "\n",
    "* The **tools** node connects back to **assistant**.\n",
    "\n",
    "* This loop continues as long as the model decides to call tools.\n",
    "\n",
    "* If the model response is not a tool call, the flow is directed to **END**, terminating the process.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418b459c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph\n",
    "builder = StateGraph(AgentState)\n",
    "\n",
    "# Define nodes: these do the work\n",
    "builder.add_node(\"assistant\", assistant)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "# Define edges: these determine how the control flow moves\n",
    "builder.add_edge(START, \"assistant\")\n",
    "builder.add_conditional_edges(\n",
    "    \"assistant\",\n",
    "    # If the latest message (result) from assistant is a tool call -> tools_condition routes to tools\n",
    "    # If the latest message (result) from assistant is a not a tool call -> tools_condition routes to END\n",
    "    tools_condition,\n",
    ")\n",
    "builder.add_edge(\"tools\", \"assistant\")\n",
    "react_graph = builder.compile()\n",
    "\n",
    "# Show\n",
    "display(Image(react_graph.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62c47a4",
   "metadata": {},
   "source": [
    "#### The Butler Langgraph Agentic Assistant In action\n",
    "\n",
    "Here is an example to show a simple use case of an agent using a tool in LangGraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea540eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"Divide 6790 by 5\")]\n",
    "\n",
    "messages = react_graph.invoke({\"messages\": messages, \"input_file\": None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b6743b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in messages['messages']:\n",
    "    m.pretty_print()S"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe23f578",
   "metadata": {},
   "source": [
    "The results:\n",
    "\n",
    "`Human`: Divide 6790 by 5\n",
    "\n",
    "`AI Tool Call`: divide(a=6790, b=5)\n",
    "\n",
    "`Tool Response`: 1358.0\n",
    "\n",
    "`Alfred`: The result of dividing 6790 by 5 is 1358.0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af96861",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# Example Building Out a Training Program\n",
    "\n",
    "obtained the figure: `Batman_training_and_meals.png` [here](https://huggingface.co/datasets/agents-course/course-images/blob/main/en/unit2/LangGraph/Batman_training_and_meals.png)\n",
    "\n",
    "\n",
    "When Master Wayne leaves his training and meal notes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f3a14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"According the note provided by MR wayne in the provided images. What's the list of items I should buy for the dinner menu ?\")]\n",
    "\n",
    "messages = react_graph.invoke({\"messages\": messages, \"input_file\": \"Batman_training_and_meals.png\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfafcf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498ef56f",
   "metadata": {},
   "source": [
    "The Results:\n",
    "\n",
    "`Human`: According to the note provided by Mr. Wayne in the provided images. What's the list of items I should buy for the dinner menu?\n",
    "\n",
    "AI Tool `Call`: extract_text(img_path=\"Batman_training_and_meals.png\")\n",
    "\n",
    "Tool Response: [Extracted `text with` training schedule `and` menu details]\n",
    "\n",
    "`Alfred`: **For** the dinner menu, you should buy the following items:\n",
    "\n",
    "1. Grass-fed local sirloin steak\n",
    "\n",
    "2. Organic spinach\n",
    "\n",
    "3. Piquillo peppers\n",
    "\n",
    "4. Potatoes (for oven-baked golden herb potato)\n",
    "\n",
    "5. Fish oil (2 grams)\n",
    "\n",
    "\n",
    "Ensure the steak is grass-fed and the spinach and peppers are organic for the best quality meal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2601bbf",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "Should you wish to create your own document analysis butler, here are key considerations:\n",
    "\n",
    "* Define clear tools for specific document-related tasks\n",
    "\n",
    "* Create a robust state tracker to maintain context between tool calls\n",
    "\n",
    "* Consider error handling for tool failures\n",
    "\n",
    "* Maintain contextual awareness of previous interactions (ensured by the operator add_messages)\n",
    "\n",
    "* With these principles, you too can provide exemplary document analysis service worthy of Wayne Manor."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-agent-hugging-face-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
